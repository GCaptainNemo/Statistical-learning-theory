{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1 Statistical Learning Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 二分类问题\n",
    "考虑最简单的二分类问题，给出如下基本定义：\n",
    "\n",
    "$$\\begin{alignat}{2}\n",
    "&回归函数(regression function) &\\qquad &\\eta(x) = P(Y=1|X) \\\\\n",
    "&分类器(classifier) &\\qquad &h:\\mathscr{X} \\rightarrow \\{0, 1\\} \\\\\n",
    "&分类误差(classification \\ error) &\\qquad &R(h) = P(h(X) \\neq Y)=\\int \\mathbb{1}(h(X) \\neq Y)dP(X, Y)\\\\\n",
    "\\end{alignat}$$\n",
    "\n",
    "在决策理论有一个基本的定理，在对分类问题所有信息已知的情况下(回归函数已知)，贝叶斯分类器(Bayes classifier)是所有分类器中分类误差最小的分类器，其它分类器与贝叶斯分类的分类误差之差称为溢出风险(excess risk)。\n",
    "\n",
    "$$\\begin{alignat}{2}\n",
    "&贝叶斯分类器 &\\qquad& h^* = \\inf_{h\\in \\Omega} R(h) = \\mathbb{1}(\\eta(X) > 1/2) \\quad (其中\\Omega为从\\mathscr{X}到\\{0, 1\\}的可测函数组成的集合) \\\\\n",
    "&溢出风险 & \\qquad & \\epsilon(h) = R(h) - R(h^*) \\\\\n",
    "\\end{alignat}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. 经验风险\n",
    "由于在一般问题中，数据分布$P(X, Y)$是未知的，只有观测数据集$D_n = \\{(x_i, y_i)\\}_{i=1,2,...,n}$</sub>(含有$P(X,Y)$的部分信息)，没法求分类器h的分类误差$R(h)$，只能求相应的经验风险，并由经验误差定义经验误差最小点(empirical risk minimizer, ERM)。\n",
    "\n",
    "$$\\begin{alignat}{2}\n",
    "&经验风险(empirical \\ risk)&\\qquad& \\hat{R}_n = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{1}(h(X) \\neq Y)\\\\\n",
    "&经验风险最小点(ERM) &\\qquad& \\hat{h}_n = \\min_{h \\in \\mathscr{H}} \\hat{R}_n(h) \n",
    "\\quad (\\mathscr{H}代表分类器的假设空间) \\\\ \n",
    "\\end{alignat}$$\n",
    "\n",
    "**注意**：经验风险是一个关于观测样本集合$D_n$的统计量，假设$D_n$中样本独立同分布，则显然经验风险是分类误差的无偏估计量，且由大数定理可知，随着n的增大，经验风险以概率1趋于分类误差。\n",
    "\n",
    "可以预料的是，经验风险最小点(ERM)不一定能泛化到训练集之外的数据。这就是大名鼎鼎的**没有免费的午餐定理**(no-free-lunch theorem)。NFL定理表明如果不对$P(X,Y)$或假设空间$\\mathscr{H}$做出任何假设，那么存在一种分布$P(X, Y)$使得贝叶斯分类器$h^*$的分类误差为0(没有噪声)，而对于经验风险最小点$\\hat{h_n}$，$\\mathbb{E}_{D_n}[R(\\hat{h_n})]$以任意精度趋于$1/2$; 而且随着$n$的增加，且存在一种分布$P(X, Y)$使得$R(h^*)=0$而$\\mathbb{E}_{D_n}[R(\\hat{h_n})]$收敛于0的速率任意慢。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 产生式模型和判别式模型\n",
    "NFL定理告诉我们需要对$P(X,Y)$进行约束(假设)，降低分布的自由度，但目前并没有清晰的结论告诉我们应该怎样选择约束。按照约束(或者说modeling)的方式可以分为产生式模型(generative model)和判别式模型(discriminative model)，有观点认为产生式模型是统计学的方法而判别式模型是机器学习的方法。\n",
    "\n",
    "产生式模型：对联合分布$P(X,Y)$进行约束，比如LDA，QDA，逻辑回归(虽然一般分入判别式模型，但可以给出一个产生式模型的解释)。\n",
    "\n",
    "判别式模型：对$P(Y|X)$进行约束或者说直接对分类器的假设空间$\\mathscr{H}$进行约束，比如神经网络、深度学习。\n",
    "\n",
    "该课程主要关注**判别式模型**的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 估计和逼近\n",
    "假设我们已经给定了一个假设空间$\\mathscr{H}$，认为在该空间中分类器的表现较好。这个假设空间可能来自对于该领域的知识，也可能仅仅是计算方便。对于从数据集获得的经验风险最小值$\\hat{h_n}$,我们可以将它的溢出风险(excess risk)分解成两部分：\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\epsilon(\\hat{h}_n) = R(\\hat{h}_n) - R(h^*) = \\underbrace{R(\\hat{h}_n) - \\inf_{h\\in\\mathscr{H}}R(h)}_{估计误差} + \\underbrace{\\inf_{h\\in\\mathscr{H}}R(h) - R(h^*)}_{逼近误差}\n",
    "\\end{equation}$$\n",
    "\n",
    "下式为逼近误差(approximation error),它的起因是因为假设空间$\\mathscr{H}$和所有分类器(可测函数)组成集合$\\Omega$之间的差距。但对于现在的深度学习、机器学习模型来说，逼近误差并不占主要的部分。比如神经网络的万能逼近定理可以保证，三层神经网络作为假设空间$\\mathscr{H}$，可以将$\\Omega$中任意一个函数逼近至任意精度。\n",
    "\n",
    "$$逼近误差: \\qquad \\inf_{h\\in\\mathscr{H}}R(h) - R(h^*)= R(\\bar{h}) - R(h^*)$$\n",
    "\n",
    "下式称为估计误差(estimation error)，它的起因是因为训练集$D_n$中只有有限的样本，只有关于分布$P(X,Y)$的部分信息。该误差可以随着$n\\rightarrow \\infty$而趋于0，但在NFL定理中我们知道，如果令$\\mathscr{H} = \\Omega$这是不会发生的，因此我们需要把$\\mathscr{H}$取得足够小，但$\\mathscr{H}$太小会导致逼近误差上升，逼近误差和估计误差之间有一个权衡(trade-off)。\n",
    "\n",
    "$$估计误差: \\qquad R(\\hat{h}_n) - \\inf_{h\\in\\mathscr{H}}R(h) = R(\\hat{h}_n) - R(\\bar{h})$$\n",
    "\n",
    "**注意**：统计学习理论更关注**估计误差**，探究估计误差如何随着变元$n$和$\\mathscr{H}$趋于0。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 估计误差\n",
    "统计学习理论主要研究**估计误差**，可以将估计误差再分解成如下两部分：\n",
    "$$ \\begin{aligned}\n",
    "\\qquad R(\\hat{h}_n) - \\inf_{h\\in\\mathscr{H}}R(h) &= R(\\hat{h}_n) - R(\\bar{h}) \\\\\n",
    "&=(R(\\hat{h}_n) - \\hat{R}_n(\\hat{h}_n)) + \\underbrace{(\\hat{R}_n(\\hat{h}_n) - \\hat{R}_n(\\bar{h}))}_{\\leq 0} + (\\hat{R}_n(\\bar{h}) -R(\\bar{h})) \\\\\n",
    "& \\leq \\vert \\hat{R}_n(\\hat{h}_n) - R(\\hat{h}_n)) \\vert + \\vert \\hat{R}_n(\\bar{h}) -R(\\bar{h}) \\vert\n",
    "\\end{aligned}$$\n",
    "\n",
    "1. 其中$\\bar{h}$是确定的(deterministic)，$\\vert \\hat{R}_n(\\bar{h}) -R(\\bar{h}) \\vert$关于$n$的界通过集中不等式(concentration inequalities)获得(比如Hoeffding不等式和Bernstein不等式)。\n",
    "\n",
    "2. 而$\\hat{R}_n(\\hat{h}_n) = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{1}(\\hat{h}_n(X_i)\\neq Y_i)$并不是独立随机变量的平均值，因为$\\hat{R}_n$是通过训练集$D_n$得到的，因此没法用集中不等式来给出它的一个上界。这时需要借用“sup-out”技术给出一个上界：\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\vert \\hat{R}_n(\\bar{h}) -R(\\bar{h}) \\vert \\leq \\sup_{h\\in \\mathscr{H}}\\vert \\hat{R}_n(\\hat{h}_n) - R(\\hat{h}_b) \\vert\n",
    "\\end{equation} $$\n",
    "\n",
    "**总结**：以上就是统计学习理论的一个主线。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
